---
title: 爬虫项目
categories:
  - 计算机项目
tags:
  - selenuim
  - 数据库
  - 爬虫
password: 3549b0028b75d981cdda2e573e9cb49dedc200185876df299f912b79f69dabd8
abbrlink: 9627
date: 2021-10-25 14:06:45
---

## 爬虫项目

### 方案

​	使用python爬虫技术，实现对百度贴吧的手机吧进行评论的`爬取`以及`分析`。

​    爬取内容举例：

   进入[百度贴吧手机吧主页](https://tieba.baidu.com/f?ie=utf-8&kw=%E6%89%8B%E6%9C%BA)，爬取该页帖子下的所有帖子url，并以此访问每一个url中帖子的每一条用户评论信息，即：`时间+楼层+用户id+文字内容`
    ![picture](/img/database1.jpg)
    ![picture](/img/database2.jpg)

### 原理

使用`python`的第三方库`selenium`、`etree`实现。

### 环境配置

首先安装如下第三方库

```python
pip install selenium 
pip install requests
pip install lxml
```

由于Selenium3.x调用浏览器必须有一个webdriver驱动文件，因此需要到[网站](https://npm.taobao.org/mirrors/chromedriver/)上下载驱动文件，注意你的浏览器和驱动器版本要匹配

设置浏览器的地址非常简单。 我们可以手动创建一个存放浏览器驱动的目录，然后打开系统变量的path，把该浏览器驱动位置写进环境变量中即可。

Selenium会控制浏览器打开[百度贴吧的手机吧](https://tieba.baidu.com/f?kw=%E6%89%8B%E6%9C%BA&ie=utf-8)页面对相关数据进行爬取。

### 爬取过程分析

#### 贴吧主页分析

首先，Selenium会控制浏览器打开[百度贴吧的手机吧](https://tieba.baidu.com/f?kw=%E6%89%8B%E6%9C%BA&ie=utf-8)页面，之后我们点击页面的检查元素：

[检查页面元素信息](/img/pachong1.jpg)

不难发现，在贴吧主页中，每一个贴吧都在一个`<div id = pagelet_frs-list/pagelet/thread_list`中，每一个帖子的具体情况被放置在各自的`li`标签中。因此，我们要首先定位这个`<div>`，并将其中的所有`<li>`标签的`<a>`的`herf`标签内容，也就是该帖子的`url`全部提取出来。

定位：使用`find_elements_by_xpath`方法，通过定位到页面底部，获取帖子数量

[提取总帖子数](/img/pachong2.jpg)

通过观察页面`url`发现，每一页会有50个帖子，每一页的`url`格式为`https://tieba.baidu.com/f?kw=%E6%89%8B%E6%9C%BA&ie=utf-8&pn=`,等号后面的数字，为本页第一个帖子的在所有主题中的序号，为此我们可以通过使用循环结构，不断便利获取每一页帖子中每个主题的`url`保存在`list`中等待使用。

在这里需要注意一个问题，百度贴吧的普通用户和`sivp`用户发的帖子主题对应的`class`是属性值是不同的，因此需要分别处理，否则会出现元素找不到程序退出的情况。

接下来，我们使用`click`方法，点击进入`list`中保存的目标网页，这里涉及到窗口的转换，使用`switch_to.window(web.window_handles[-1]) ` 方法，将页面控制跳转到第二个页面，当第二个页面(也就是帖子内容页面)，爬取完成后，，使用close()方法关闭第二个页面，同时使用`switch_to.window(web.window_handles[0])`方法返回第一个页面进行控制。

#### 贴吧主题内容分析

进入每一个页面后，我们需要爬取的内容为：在该页面发表评论的用户id、该用户发表的内容和，以及帖子发表的时间具体时间三个信息。

首先我们创建一个名字为该主题的文件，该文件用来保存该主题下所有要爬取的文件信息，接下来，我们开始通过定位，获取页面内容：

1. 第一步是获取该主题的总页数
2. 第二步是根据总页数，通过循环结构获取每一页用户以及发表内容的信息

值得注意的，有的帖子虽然在贴吧主页可以爬取到它的`url`,实际上它可能不存在，因此我们需要添加一个检测机制，避免因找不要元素程序停止。通过分析发现，`404`页面的`body`标签的`class`属性含有字符串`page404`，因此只要有页面中出现这个字符串，就直接结束函数回到贴吧主页。

在对用户评论进行爬取时，发现了用户评论时间百度贴吧使用了懒加载技术，因此需要使用js设置相关命令，让控制器等待页面内容加载完毕后再进行爬取，以防止程序爬不到元素内容而停止运行。

懒加载配置如下

```python
 js = "window.scrollBy(0,1000)"
 web.execute_script(js)
```

另外需要注意的是，当我们发现用户`name`是`百度市场`时，自动略过此广告进行对下一个用户内容的爬取。

我们每爬取一个用户的内容，就将其写入文件之中，注意只有执行`close()`函数后，数据才能真正保存在文件中。

#### 反爬虫机制

在本次爬虫项目中，百度贴吧具有很强的反爬虫机制：一般伪装智只会返回`<scripy>`标签的内容、爬取时间过长、爬取速度过快，都会被封禁`ip`。本1项目主要采用以下方法进行反爬虫：

1. 使用`selenium`的爬虫方法，模拟人类的操作，大大降低了被封禁的概率
2. 由于是可视化，因此可以在正式提取信息前进行真实用户登录
3. 每爬取一个用户信息休息1s
4. 每爬取一部分主题后，就返回贴吧主页休息几秒再进行爬取

### 总结与展望

​	本次爬虫内容，使用了`selenium `的一些基本方法，实现了对百度贴吧的爬取，通过使用相关控制命令，有效解决了懒加载页面机制，通过使用一些休眠和页面跳转的技巧，有效避免了被服务器封禁的可能。当然，还有许多网页拥有更加复杂的反爬虫机制，未来的爬虫之路任重而道远。

### 源代码

```python
from os import pipe
from selenium import webdriver
from selenium.webdriver.chrome.webdriver import WebDriver
from selenium.webdriver.common.keys import Keys
import time
import requests
from lxml import etree
#encoding: utf-8
def create_ip_pool():
    headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36'
    }
    fp = open("ipPool.txt",'w',encoding= 'utf-8')
    print("==========================正在构造IP池===================================")
    for i in range(1,2001):
        url = 'http://www.xiladaili.com/gaoni/'+str(i)+'/'
        respones = requests.get(url=url,headers=headers).text
        tree = etree.HTML(respones)
        tree_list_gao_ni = tree.xpath('//tbody/tr/td[1]/text()')
        for i in range(0,len(tree_list_gao_ni)):
            proxy = tree_list_gao_ni[i]	
            proxies = {
	        'http': 'http://' + proxy
		    }
            print('==========================检测IP是否可用===================================')
            try:
                is_valid = requests.get('https://www.baidu.com/',headers=headers,proxies=proxies)
                if len(is_valid.content) > 100:
                    print('当前代理可用:'+tree_list_gao_ni[i])
                    fp.write(tree_list_gao_ni[i]+'\n')
            except Exception:
                print('不可用ip代理'+tree_list_gao_ni[i])
            finally:
                print('当前代理'+tree_list_gao_ni[i]+'检测完毕')
    fp.close()   
    print('==========================IP池构造完毕===================================') 
def init():
    # 使用selenium
    #opt = webdriver.ChromeOptions()
    #opt .add_argument("–proxy-server=http://186.29.163.97:49787")# 一定要注意，等号两边不能有空格
    web = webdriver.Chrome()
    web.get('https://tieba.baidu.com/f?ie=utf-8&kw=%E6%89%8B%E6%9C%BA&fr=search')
    #time.sleep(30)
    return web
def get_all_link(web):
    # 获取帖子总数
    n = web.find_elements_by_xpath('//*[@id="pagelet_frs-list/pagelet/thread_list"]/div/div[2]/div/span')[0]
    number = int(n.text)
    print(number)
    # 执行获取每个帖子链接
    counts = 0
    for m in range(0,number,50):
        counts += 1
        if(counts % 10 == 0):
            print('Have a rest')
            time.sleep(10)
        li_list = web.find_elements_by_xpath('//div[@id="pagelet_frs-list/pagelet/thread_list"]//li[@class=" j_thread_list clearfix thread_item_box"]')
        # 处理本页的每一个帖子
        count = 1
        for i in li_list:
            try:
                # 如果用户不是vip
                s = i.find_element_by_xpath('./div[@class="t_con cleafix"]//div[@class="threadlist_title pull_left j_th_tit "]/a')
            except:
                try:
                # 如果用户是vip
                    s = i.find_element_by_xpath('./div[@class="t_con cleafix"]//div[@class="threadlist_title pull_left j_th_tit  member_thread_title_frs "]/a')
                except:
                    continue
            
            # 获取帖子地址以及标题
            url   = s.get_attribute("href")
            info  = s.text
            # 输出信息
            print(url+ ":"+info)
            #对每一个帖子进行处理
            #跳转目标页面
            
            try:
                s.click()
                web.switch_to.window(web.window_handles[-1])
                every_info(web,info)
                web.close()
                web.switch_to.window(web.window_handles[0])
                if count == len(li_list):
                    break
            except:
                continue
            print(len(li_list))
            print(count)
            count += 1
        next = "https://tieba.baidu.com/f?kw=%E6%89%8B%E6%9C%BA&ie=utf-8&pn="+str(m)
        print("=========================================================================================================")
        web.get(next)
        print(web.current_url)
def every_info(web,info): 
     next_url_save = web.current_url     
     s = web.find_element_by_xpath('//body').get_attribute("class")
     if s == 'page404':
         return
     # 首先确定每个帖子的页数
     n = web.find_elements_by_xpath('//li[@class="l_reply_num"]/span')[3]
     num = int(n.text)
     # 提取贴吧主题
     tie_themem =  web.find_elements_by_xpath('//*[@id="j_core_title_wrap"]/div[2]/h1')[0].text
     print(tie_themem)
     # 对帖子所有页进行信息提取
     try:
         m = open(info,'r')
         return
     except:
         fp = open(info,'w',encoding='utf-8')
     for page in range(1,num):
        lists = web.find_elements_by_xpath('//div[@id="j_p_postlist"]/div')
        js = "window.scrollBy(0,1000)"
        for i in range(0,len(lists)):
            if i > 0 :
                fp = open(info,'a',encoding='utf-8')
        # 主回复
            web.execute_script(js)
            tie__name = lists[i].find_element_by_xpath('.//li[@class="d_name"]/a').text
            if tie__name == "百度AI市场":
                continue
            '''
            <ul class="p_tail"><li><span>1楼</span></li><li><span>2021-09-21 13:39</span></li></ul>
            '''
            try:
                tie__time = lists[i].find_element_by_xpath('.//ul[@class="p_tail"]/li[2]/span').text
                #print("回复时间是"+tie__time)
                tie__floor = lists[i].find_element_by_xpath('.//ul[@class="p_tail"]/li[1]/span').text
                #print("回复楼层是"+tie__floor)
                tie__content = lists[i].find_element_by_xpath(
                    './/div[@class="d_post_content j_d_post_content  clearfix"]').text
                print(tie__time[0:-1]+tie__floor+tie__name+':'+tie__content)
                fp.write(tie__time)
                fp.write('-')
                fp.write(tie__floor)
                fp.write('-')
                fp.write(tie__name)
                fp.write(':')
                fp.write(tie__content)
                fp.write('\n')
            except:
                web.execute_script("window.scrollBy(0,300)")            
         # 继续爬取下一页帖子信息
        fp.close()
        next_url =  next_url_save + '?pn='+str(page)
        web.get('https://tieba.baidu.com/')
        web.get(next_url)
        time.sleep(1)
     return 
         
def scroll_foot(self):
    '''
    滚动条拉到底部
　　:return:
　　'''
    js = "var q=document.documentElement.scrollTop=300"
    #将滚动条移动到页面的顶部
    # js="var q=document.documentElement.scrollTop=0"
    return self.driver.execute_script(js)
def changeIP(web,fp_ip):
        time.sleep(2)
        current_url =  web.current_url
        opt = webdriver.ChromeOptions()
        opt .add_argument("–proxy-server=http://"+fp_ip.readline())# 一定要注意，等号两边不能有空格
        web = webdriver.Chrome(options=opt)
        web.get(current_url) 
if __name__ == '__main__':
    # 构造ip池
    #create_ip_pool()
    get_all_link(init())



#web.set_proxy(web, http_addr="212.35.56.22", http_port=8888)
```

key ： database
