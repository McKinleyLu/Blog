---
title: 散度
author: 瑾年
toc: true
mathjax: true
summary: KL散度、JS散度以及Wasserstein距离、Bregman距离解读
categories: 计算机进阶知识
tags:
  - 散度
abbrlink: 45786
date: 2021-09-03 10:43:22
---
### 目标分类损失函数

1. 损失函数的功能是通过`样本`来计算`模型分布`与`目标分布`间的差异。
2. 在分布差异计算中,KL散度是最合适的。
3. 交叉熵 ：其用来衡量在给定的真实分布下，使用非真实分布所指定的策略`消除系统的不确定性`所需要付出的努力的大小。
4. 交叉熵 = KL散度 裁剪  目标分布熵   因此只需要计算交叉熵就可以得到模型分布与目标分布的损失值。
5. 相对熵(KL散度)：衡量不同策略之间的差异呢，所以我们使用KL散度来做模型分布的拟合损失。

### 信息量

  任何事件都会承载着一定的信息量，包括已经发生的事件和未发生的事件，只是它们承载的信息量会有所不同。

   已经发生的事情，其信息量为0，如果未发生，其信息量就大。

  我们已知某个事件的信息量是与它发生的概率有关，那我们可以通过如下公式计算信息量。
   假设X是一个离散型随机变量，其取值集合为X，概率分布函数为$p(x) = P_r(X = x),x \in X$,其定义事件$X = x_0$的信息量为：$I(x_0) = -log(p(x_0))$

即一个事件发生概率为$p(x)$,那么它的信息量为$-log(p(x))$

### 熵

即信息量的期望，把这一事件所有的可能都罗列出来。

假设事件X一共有n种可能，发生$x_i$的概率为$p(x_i)$，那么事件的熵$H(X)$为:$H(X) = - \Sigma p(x_i)log(p(x_i)) = -p(x)log(p(x)) - (1 - p(x))log(1-p(x))$

### KL散度

对于同一个随机变量 x 有两个单独的概率分布 P$(x)$ 和 Q$(x)$，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异。

在机器学习中，P往往用来表示样本的真实分布，Q用来表示模型所预测的分布，那么KL散度就可以计算两个分布的差异，也就是Loss损失值:

D<sub>KL</sub> $(p || q) = \Sigma p(x_i)log(\frac{p(x_i)}{q(x_i)})$

由于-log(u)是凸函数，因此有下面的不等式 

D<sub>KL</sub>$(q || p) = - \Sigma x \in X  q(x)log[\frac{p(x_i)}{q(x_i)}] = Elog[\frac{p(x_i)}{q(x_i)}]≥ -logE[\frac{p(x_i)}{q(x_i)}] = - log \Sigma x \in X  q(x)\frac{p(x_i)}{q(x_i)} = 0 $

  即KL-divergence始终是大于等于0的。当且仅当两分布相同时，KL-divergence等于0。

#####  性质

* 不对称性  

  尽管KL散度从直观上是个度量或距离函数，但它并不是一个真正的度量或者距离

  $D(p || q) \neq D(q || p) $

* 相对熵的值为非负值

  $D(p || q) \geq 0$

### JS散度

由于KL散度的不对称问题使得在训练过程可能存在一些问题，为了解决这个问题，在KL问题基础上提出了JS散度

JS散度是对称的，其取值在0到1之间。如果两个分布P、Q离得很远，完全没有重叠时，那么KL散度值是没有意义的，而JS散度时一个常数。接就意味着这一点梯度为0，梯度消失了。

推导当完全没有重叠时，log2是如何得到的：

![pircture2](/img/m07.jpg)  

当两个分布的中心距离有多近，其JS散度都是一个常数，以至于梯度为0，无法更新。

### Wasserstein距离

wasserstein距离的优势在于，相比KL和JS散度，即便两个分布没有重叠，wasserstein距离依旧可以反映出它们的远近。

wasserstein距离度量两个概率分布之间的距离，定义如下：

​     ![公式1](/img/m03.jpg)

其中，Π(P1,P2)是P1和P2分布组合起来的所有可能的联合分布的集合。对于每一个可能的联合分布γ，可以从中采样(x,y)∼γ得到一个样本x和y，并计算出这对样本的距离||x−y||，所以可以计算该联合分布γ下，样本对距离的期望值E(x,y)∼γ[||x−y||]。在所有可能的联合分布中能够对这个期望值取到的下界infγ∼Π(P1,P2)E(x,y)∼γ[||x−y||]就是Wasserstein距离。

通过如下例子，分析这三个散度：

二维空间中的两个分布$p_1和p_2$,$p_1$在线段AB上均匀分布，$p_2$在线段CD上均匀分布，通过控制参数$\theta$可以控制两个分布距离的远近：

 ![picture](/img/m04.jpg)

可以得到如下分段函数：

![函数1](/img/m05.svg)   

![函数2](/img/m06.svg)    

![函数3](/img/m07.svg)

KL散度和JS散度是突变的，要么最大要么最小，Wasserstein距离却是平滑的，如果我们要用梯度下降法优化$\theta$这个参数，前两者根本提供不了梯度，Wasserstein距离却可以。类似地，在高维空间中如果两个分布不重叠或者重叠部分可忽略，则KL和JS既反映不了远近，也提供不了梯度，但是Wasserstein却可以提供有意义的梯度。

### Bregman距离

Bregman散度也是一个类似于距离度量的方式，用于衡量两者之间的差异大小。

Bregman散度可以理解为损失或者失真函数。

考虑如下情况：设点 *p* 是点 *q* 的失真或者近似的点，也就是说可能 *p* 是由 *q* 添加了一些噪声形成的，损失函数的目的是度量 *p* 近似 *q* 导致的失真或者损失，因而Bregman散度可以用作相异性函数。

##### 公式定义

定义函数F：$\Omega$ $\rightarrow$R。 其中，$\Omega$是一个凸集，F是一个严格凸二次可微。

由该函数F生成Bregman散度的公式如下

$D_f(p \cdot q) = F(p) - F(q) - ⟨∇F(q),(p−q)⟩$

其中,∇F(q)表示函数F在q处的梯度，(p−q)表示两个向量的差，⟨∇F(q),(p−q)⟩是∇F(q)和(p−q)的内积。

以上公式的后半部分$L(p,q)=F(q)+⟨∇F(q),(p−q)⟩L(p,q)=F(q)+⟨∇F(q),(p−q)⟩$表示了函数F在q点附近的线性部分，而Bregman散度是一个函数与该函数的线性近似（一阶Taylor展开）之间的差，选取不同的函数F可以得到不同的Bregman散度。

注：泰勒一阶展开式$(f(x) = f(0) + f'(0) * \Delta x)$

#### 性质

* 不满足三角不等式:即对任意的x、y、z，$D_F(x,z)≤D_F(x,ｙ)＋D_F(ｙ,z)$不一定成立

* 不满足对称性:即对任意ｘ和ｙ   $D_F(x,y)=D_F(y,x)$不一定成立

* 非负性

* 凸性: $D_F(p,q)$在第一个参数上是凸的,但是在第二个参数上不一定是

* 线性：对于严格凸且可微的函数$F_1和 F_2$，以及系数 λ≥0，满足D<sub>$F_1$+$F_2$<sub> 

  = D<sub>$F_1$<sub>(p,q)

  +λD<sub>$F_2$<sub>(p,q)

* 对偶性: 函数F具有凸的共轭$F^*$,则$F^*$的Bregman散度与$D_F(p,q)$存在如下联系：

  ![pircture2](/img/m06.jpg)  



  

